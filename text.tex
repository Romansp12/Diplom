\documentclass[a4paper, 14pt]{extarticle}
\usepackage[left=3cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[T1, T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{listings}

\newtheorem{lemma}{Лемма}[section]
\newtheorem{defn}{Определение}[section]
\newtheorem{thr}{Теорема}[section]

\linespread{1.3}

\begin{document}
	
	\tableofcontents
		
	\newpage
	
	
	\section*{Аннотация}
		Данная работа посвещана применению методов машинного обучения в задачах классификации текстов. В частности, рассматривается задача фильтрации спама. Основной задачей данной работы являлось исследование основных решений и их сравнительный анализ. 
	
	\newpage
	
	\section{Введение}
        Доля спама в почтовом трафике состовляет более 60\%. Поэтому это очень важная проблема. Те почтовые сервисы, которые смогут лучше отфильтровать спам для своего пользователя, те и будут более востребованы. Мы рассмотрим как можно подойти к этой задаче методами машинного обучения.\newline
        Машинное обучение — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи, а обучение в процессе применения решений множества сходных задач.
        В данном случае, рассматривается задача классификации, предсказание принадлежности объекта к одному из выделенных классов.
        У нас есть множество писем $A$ и мы хотим научится отображать его в двухэлементное множество(спам/не спам): 
        $F : A \to \{0, 1\}$.\newline
        Пусть нам дана выборка $U\subset A$  писем, у которых известна их классификация. То есть задана функция $G : U \to \{0, 1\}$. Наша цель построить алгоритм $F : A \to \{0, 1\}$, который приближал бы неизвестную целевую зависимость как на элементах $U$, так и на всём множестве $A$. Мы рассмотрим несколько алгоритмов решающих эту задачу и нам нужно их сравнивать между собой. Для этого 
        рассмотрим следующие метрики качества этих алгоритмов 
        
    \section{Методы решения задачи}
        \subsection{Наивный байесовский классификатор}	
	    Рассморим все письма из обучающего множества $U$.В нашей выборке все слова в письмах приведены в начальную форму и удалены служебные слова. Произвольное письмо $d\in U$, состоит из слов ($ d_1,d_2,d_3...d_n$). Для каждого слова w, посчитаем $n_{ws}$ и $n_{wh}$,то есть количество спам-писем и неспам-писем в которых оно присутствует. Отсюда можно оценить вероятность появления каждого слова w в спамном и неспамном письме.Всего количество писем содержащих слово s обозначим за $n_{s}$.
	    \newpage
	    \begin{center}
	        $P(w|spam)=n_{ws}/n_s$\\
	        $P(w|nospam)=n_{wh}/n_s$
	    \end{center}
	    Если у нас есть новое письмо, которое нужно классифицировать. Мы можем оценить вероятности появления всего текста в классе спам и неспам произведением вероятностей слов входящих в него. Алгоритм потому и называется наивным, потому что предполагает незавимость появления слов в тексте. 
	    \begin{center}
	        $P(d|spam)=P(d_1|spam)P(d_2|spam)P(d_3|spam)..P(d_n|spam)$
	        $P(d|nospam)=P(d_1|nospam)P(d_2|nospam)P(d_3|nospam)..P(d_n|nospam)$
	    \end{center}
	    Пускай $y\in\{spam,nospam\}$ какой то из классов. Воспользуемся теоремой Байеса.
	    \begin{center}
	        $P(y|d)=P(d|y)P(y)/P(d)$
	    \end{center}
	    Так как $P(d)$ одинаковая в обоих случаях, то
	    нам нужно просто посчитать, что больше $P(d|spam)P(spam)$ или $P(d|nospam)P(nospam)$, чья вероятность больше к тому классу мы и классифицируем. Однако сразу заметим, что если слово $d_i$ не встречалось в обучающей выборке у spam или nospam, соответствующая для него вероятность будет равна нулю и все выражение будет равно нулю. Для этого необходимо применить сглаживание, а точнее – сделать небольшие поправки во все вероятности вхождения слов в документ. Пускай $0\le\alpha\le1$, вместо $P(d_i|y)$ будем считать $(\alpha+{n_{d_{i}y}})/(\alpha*|U|+n_{d_i})$ . Также заметим, что произведение этих вероятностей может быть совсем маленькими,поэтому можно сравнивать логарифмы этих выражений и это приведет к большей точности.
	    
	    
\end{document}
